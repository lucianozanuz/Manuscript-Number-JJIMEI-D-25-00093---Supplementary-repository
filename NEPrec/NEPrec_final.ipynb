{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WPYkeEMywxqO",
        "outputId": "72a2e66a-009e-4e27-f364-69d069c95b39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting pt-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.8.0/pt_core_news_lg-3.8.0-py3-none-any.whl (568.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.2/568.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pt-core-news-lg\n",
            "Successfully installed pt-core-news-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download pt_core_news_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f8Z0dmIuw4xB"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"pt_core_news_lg\")\n",
        "\n",
        "LEGAL_ENTITY_TYPES = {\n",
        "    \"PER\",   # Pessoas\n",
        "    \"ORG\",   # Órgãos, tribunais\n",
        "    \"LOC\",   # Localização\n",
        "    \"DATE\",  # Datas\n",
        "    \"MONEY\"  # Valores monetários\n",
        "}\n",
        "\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    return [\n",
        "        ent.text.strip().lower()\n",
        "        for ent in doc.ents\n",
        "        if ent.label_ in LEGAL_ENTITY_TYPES\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nSlW9nQ7w86h"
      },
      "outputs": [],
      "source": [
        "def compute_neprec(reference_text, generated_text):\n",
        "    ref_entities = set(extract_entities(reference_text))\n",
        "    gen_entities = extract_entities(generated_text)\n",
        "\n",
        "    if len(gen_entities) == 0:\n",
        "        return 1.0  # No hallucination if no entities used\n",
        "\n",
        "    correct = sum(1 for ent in gen_entities if ent in ref_entities)\n",
        "    precision = correct / len(gen_entities)\n",
        "    return precision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1R5XIwzxODh",
        "outputId": "d8487303-5683-4f04-946a-c7db3343f105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset carregado. Total de linhas: 210\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "file_path = 'hallucination_analysis.xlsx'\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"ERRO: O arquivo {file_path} não foi encontrado. Por favor, faça o upload no menu lateral esquerdo.\")\n",
        "else:\n",
        "    # 1. Carregar o dataset\n",
        "    df = pd.read_excel(file_path)\n",
        "    print(f\"Dataset carregado. Total de linhas: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8JORDUlxAf7",
        "outputId": "dce2f909-0f69-44c8-d092-32c9e308c50e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         model_id      mean       std\n",
            "0  gpt-4o-mini-v2  0.289932  0.254560\n",
            "1       gpt-4o-v2  0.231257  0.185116\n"
          ]
        }
      ],
      "source": [
        "df[\"neprec\"] = df.apply(\n",
        "    lambda row: compute_neprec(row[\"reference_text\"], row[\"generated_text\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "neprec_results = (\n",
        "    df.groupby(\"model_id\")[\"neprec\"]\n",
        "      .agg([\"mean\", \"std\"])\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "print(neprec_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwrxdUqW1_vu",
        "outputId": "df8d6151-ae0f-48e7-ed27-be1edfcc5616"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.12.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Collecting azure-identity\n",
            "  Downloading azure_identity-1.25.1-py3-none-any.whl.metadata (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-core>=1.31.0 (from azure-identity)\n",
            "  Downloading azure_core-1.37.0-py3-none-any.whl.metadata (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.12/dist-packages (from azure-identity) (43.0.3)\n",
            "Collecting msal>=1.30.0 (from azure-identity)\n",
            "  Downloading msal-1.34.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msal-extensions>=1.2.0 (from azure-identity)\n",
            "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from azure-identity) (4.15.0)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from azure-core>=1.31.0->azure-identity) (2.32.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=2.5->azure-identity) (2.0.0)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity) (2.10.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (2025.11.12)\n",
            "Downloading azure_identity-1.25.1-py3-none-any.whl (191 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.3/191.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.37.0-py3-none-any.whl (214 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.0/214.0 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal-1.34.0-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: azure-core, msal, msal-extensions, azure-identity\n",
            "Successfully installed azure-core-1.37.0 azure-identity-1.25.1 msal-1.34.0 msal-extensions-1.3.1\n",
            "Requirement already satisfied: azure-core in /usr/local/lib/python3.12/dist-packages (1.37.0)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from azure-core) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from azure-core) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core) (2025.11.12)\n",
            "AzureOpenAI client defined.\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install azure-identity\n",
        "!pip install azure-core\n",
        "\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "endpoint = userdata.get('MODEL_ENDPOINT')\n",
        "deployment = userdata.get('MODEL_DEPLOY')\n",
        "\n",
        "subscription_key = userdata.get('MODEL_KEY')\n",
        "api_version = userdata.get('MODEL_API_VERSION')\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_version=api_version,\n",
        "    azure_endpoint=endpoint,\n",
        "    api_key=subscription_key,\n",
        ")\n",
        "\n",
        "print(\"AzureOpenAI client defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70b8e423",
        "outputId": "d01045de-0ee0-48e3-e3f5-a604ae422811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt content template for AzureOpenAI client defined.\n",
            "Prompt content template for LLM coverage comparison defined.\n"
          ]
        }
      ],
      "source": [
        "llm_prompt_content_template = \"\"\"Você é um avaliador de texto inteligente. Sua tarefa é comparar duas listas de entidades extraídas de documentos legais: 'entidades de referência' e 'entidades geradas'.\n",
        "\n",
        "As 'entidades de referência' representam as entidades corretas e esperadas.\n",
        "As 'entidades geradas' são as entidades que foram extraídas por um modelo de linguagem.\n",
        "\n",
        "Sua avaliação deve ser baseada na precisão semântica e contextual das 'entidades geradas' em relação às 'entidades de referência'. Ou seja, quão bem as entidades geradas correspondem, em significado e relevância para o contexto legal, às entidades de referência.\n",
        "\n",
        "Retorne um único valor float entre 0.0 e 1.0, onde:\n",
        "- 1.0 significa que todas as entidades geradas são semanticamente corretas e altamente relevantes em comparação com as de referência.\n",
        "- 0.0 significa que nenhuma das entidades geradas é semanticamente correta ou relevante.\n",
        "- Valores intermediários representam precisão parcial.\n",
        "\n",
        "Considere sinônimos, termos relacionados e o contexto legal para determinar a precisão. Se uma entidade gerada é ligeiramente diferente, mas ainda assim semanticamente a mesma, considere-a parcialmente correta.\n",
        "\n",
        "Entidades de Referência: {reference_entities}\n",
        "Entidades Geradas: {generated_entities}\n",
        "\n",
        "Retorne apenas o valor float, sem nenhum texto adicional. Por exemplo: 0.75\n",
        "\"\"\"\n",
        "\n",
        "print(\"Prompt content template for AzureOpenAI client defined.\")\n",
        "\n",
        "llm_coverage_prompt_template = \"\"\"Você é um avaliador de texto inteligente. Sua tarefa é avaliar a 'cobertura' das 'entidades de referência' pelas 'entidades geradas'.\n",
        "As 'entidades de referência' são as entidades esperadas e cruciais. As 'entidades geradas' são as entidades que foram extraídas por um modelo de linguagem, e tendem a ser mais abrangentes.\n",
        "\n",
        "Você deve determinar qual proporção das 'entidades de referência' é semanticamente coberta ou 'contemplada' pelas 'entidades geradas'.\n",
        "- Para cada entidade na lista de 'entidades de referência', verifique se existe uma entidade semanticamente equivalente ou um conceito relacionado na lista de 'entidades geradas'.\n",
        "- Retorne um único valor float entre 0.0 e 1.0, onde:\n",
        "- 1.0 significa que todas as entidades de referência são semanticamente cobertas pelas entidades geradas.\n",
        "- 0.0 significa que nenhuma das entidades de referência é semanticamente coberta.\n",
        "- Valores intermediários representam cobertura parcial.\n",
        "\n",
        "Considere sinônimos, termos relacionados e o contexto legal para determinar a cobertura.\n",
        "\n",
        "Entidades de Referência: {reference_entities}\n",
        "Entidades Geradas: {generated_entities}\n",
        "\n",
        "Retorne apenas o valor float, sem nenhum texto adicional. Por exemplo: 0.85\n",
        "\"\"\"\n",
        "\n",
        "print(\"Prompt content template for LLM coverage comparison defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37e0e888",
        "outputId": "73855f43-6997-4ec5-89ee-33fa724426bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function `compute_llm_neprec_coverage` defined.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def compute_llm_neprec_coverage(reference_entities: list, generated_entities: list) -> float:\n",
        "    # Se a lista de entidades de referência estiver vazia, a cobertura é 1.0 (não há nada para cobrir)\n",
        "    if not reference_entities:\n",
        "        return 1.0\n",
        "\n",
        "    # Se a lista de entidades geradas estiver vazia e há entidades de referência, a cobertura é 0.0\n",
        "    if not generated_entities and reference_entities:\n",
        "        return 0.0\n",
        "\n",
        "    # Convert lists to string representations suitable for the prompt\n",
        "    ref_entities_str = \", \".join(reference_entities)\n",
        "    gen_entities_str = \", \".join(generated_entities)\n",
        "\n",
        "    # Format the prompt using the global template\n",
        "    prompt_content = llm_coverage_prompt_template.format(\n",
        "        reference_entities=ref_entities_str,\n",
        "        generated_entities=gen_entities_str\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Invoque o LLM com o prompt formatado usando AzureOpenAI client\n",
        "        response = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are an intelligent text evaluator. Always respond with a float value.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt_content\n",
        "                }\n",
        "            ],\n",
        "            max_completion_tokens=50,\n",
        "            temperature=0.0,\n",
        "            model=deployment\n",
        "        )\n",
        "        llm_response = response.choices[0].message.content\n",
        "\n",
        "        # Processar a resposta do LLM, extraindo o valor float retornado\n",
        "        match = re.search(r\"\\d+\\.\\d+\", llm_response)\n",
        "        if match:\n",
        "            score = float(match.group(0))\n",
        "            # Garantir que o score esteja dentro do range válido [0.0, 1.0]\n",
        "            return max(0.0, min(1.0, score))\n",
        "        else:\n",
        "            print(f\"Warning: LLM did not return a valid float for coverage. Response: {llm_response}\")\n",
        "            return 0.0 # Default para 0 se o parsing falhar\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM coverage invocation: {e}\")\n",
        "        return 0.0 # Retorna 0 em caso de erro\n",
        "\n",
        "print(\"Function `compute_llm_neprec_coverage` defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdac69c3",
        "outputId": "e09f4270-7e27-4104-c56f-0cc6e8a211c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function `compute_llm_neprec_smart_compare` defined.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def compute_llm_neprec_smart_compare(reference_entities: list, generated_entities: list) -> float:\n",
        "    # Se a lista generated_entities estiver vazia, retorne 1.0\n",
        "    if not generated_entities:\n",
        "        return 1.0\n",
        "\n",
        "    # Utilizar o PromptTemplate definido para formatar as listas de entidades\n",
        "    # Convert lists to string representations suitable for the prompt\n",
        "    ref_entities_str = \", \".join(reference_entities)\n",
        "    gen_entities_str = \", \".join(generated_entities)\n",
        "\n",
        "    # Format the prompt using the global template\n",
        "    prompt_content = llm_prompt_content_template.format(\n",
        "        reference_entities=ref_entities_str,\n",
        "        generated_entities=gen_entities_str\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Invoque o LLM com o prompt formatado usando AzureOpenAI client\n",
        "        response = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are an intelligent text evaluator. Always respond with a float value.\",\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt_content,\n",
        "                }\n",
        "            ],\n",
        "            max_completion_tokens=50, # Limiting tokens as only a float is expected\n",
        "            temperature=0.0,\n",
        "            model=deployment # Use the deployment model from the client setup\n",
        "        )\n",
        "        llm_response = response.choices[0].message.content\n",
        "\n",
        "        # Processe a resposta do LLM, extraindo o valor float retornado\n",
        "        # Use regex to find a float value in the response\n",
        "        match = re.search(r\"\\d+\\.\\d+\", llm_response)\n",
        "        if match:\n",
        "            score = float(match.group(0))\n",
        "            # Ensure the score is within the valid range [0.0, 1.0]\n",
        "            return max(0.0, min(1.0, score))\n",
        "        else:\n",
        "            print(f\"Warning: LLM did not return a valid float. Response: {llm_response}\")\n",
        "            return 0.0 # Default to 0 if parsing fails\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM invocation: {e}\")\n",
        "        return 0.0 # Return 0 in case of an error\n",
        "\n",
        "print(\"Function `compute_llm_neprec_smart_compare` defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "55c81910",
        "outputId": "3668fe06-3f11-497c-e2e4-ce1651766c3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function `extract_entities_llm` and its prompt defined.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import ast # Required for safer parsing of LLM output\n",
        "\n",
        "llm_extract_prompt_template = \"\"\"Você é um assistente de IA especialista em extração de entidades legais de texto em Português do Brasil.\n",
        "Sua tarefa é identificar e listar todas as entidades legais relevantes no texto fornecido.\n",
        "Considere as seguintes categorias de entidades legais:\n",
        "- PER (Pessoas, nomes de indivíduos)\n",
        "- ORG (Órgãos, empresas, tribunais, organizações)\n",
        "- LOC (Localidades, endereços, cidades, países)\n",
        "- DATE (Datas, períodos)\n",
        "- MONEY (Valores monetários)\n",
        "\n",
        "Liste as entidades no formato de lista Python de strings, onde cada string é a entidade encontrada. Garanta que cada entidade esteja em minúsculas e sem espaços extras.\n",
        "Se nenhuma entidade for encontrada, retorne uma lista vazia `[]`.\n",
        "\n",
        "Texto: {text}\n",
        "\n",
        "Entidades extraídas (apenas a lista Python):\"\"\"\n",
        "\n",
        "# No llm_extract_chain needed for direct AzureOpenAI client calls\n",
        "\n",
        "def extract_entities_llm(text: str) -> list:\n",
        "    try:\n",
        "        # Format the prompt for the AzureOpenAI client\n",
        "        prompt_content = llm_extract_prompt_template.format(text=text)\n",
        "\n",
        "        # Invoke the LLM with the formatted prompt using AzureOpenAI client\n",
        "        response = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are an AI assistant for legal entity extraction. Always respond with a Python list of strings, e.g., ['entity 1', 'entity 2'].\",\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt_content,\n",
        "                }\n",
        "            ],\n",
        "            max_completion_tokens=1000, # Increased tokens for extraction\n",
        "            temperature=0.0,\n",
        "            model=deployment # Use the deployment model from the client setup\n",
        "        )\n",
        "        llm_response = response.choices[0].message.content\n",
        "\n",
        "        # Attempt to parse the response as a Python list\n",
        "        # Using ast.literal_eval for safer parsing than eval()\n",
        "        entities = ast.literal_eval(llm_response)\n",
        "        if isinstance(entities, list):\n",
        "            # Convert all entities to lowercase for consistency\n",
        "            return [str(entity).strip().lower() for entity in entities]\n",
        "        else:\n",
        "            print(f\"Warning: LLM did not return a valid list for extraction. Response: {llm_response}\")\n",
        "            return []\n",
        "    except (SyntaxError, ValueError) as e:\n",
        "        print(f\"Error parsing LLM response as a list: {e}. Response: {llm_response}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM entity extraction: {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"Function `extract_entities_llm` and its prompt defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f9b67fe",
        "outputId": "bc17f853-2115-4978-bee2-8b851f1ed079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function `compute_llm_neprec_coverage_llm_extracted_entities` defined.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def compute_llm_neprec_coverage_llm_extracted_entities(reference_text: str, generated_text: str) -> float:\n",
        "    # 1. Extrair entidades do texto de referência usando o LLM\n",
        "    ref_entities_llm = extract_entities_llm(reference_text)\n",
        "\n",
        "    # 2. Extrair entidades do texto gerado usando o LLM\n",
        "    gen_entities_llm = extract_entities_llm(generated_text)\n",
        "\n",
        "    # 3. Calcular a cobertura usando a função de cobertura com LLM-extracted entities\n",
        "    # A função compute_llm_neprec_coverage já lida com listas vazias de generated_entities\n",
        "    llm_coverage_score = compute_llm_neprec_coverage(ref_entities_llm, gen_entities_llm)\n",
        "\n",
        "    return llm_coverage_score\n",
        "\n",
        "print(\"Function `compute_llm_neprec_coverage_llm_extracted_entities` defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b626648a",
        "outputId": "9e22a886-f2c1-46b6-dbc3-77150ed4f3d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function `compute_llm_neprec_extract_and_compare` defined.\n"
          ]
        }
      ],
      "source": [
        "def compute_llm_neprec_extract_and_compare(reference_text: str, generated_text: str) -> float:\n",
        "    # 1. Extrair entidades do texto de referência usando o LLM\n",
        "    ref_entities_llm = extract_entities_llm(reference_text)\n",
        "\n",
        "    # 2. Extrair entidades do texto gerado usando o LLM\n",
        "    gen_entities_llm = extract_entities_llm(generated_text)\n",
        "\n",
        "    # 3. Calcular a precisão semântica usando a função de comparação com LLM\n",
        "    # A função compute_llm_neprec_smart_compare já lida com listas vazias de generated_entities\n",
        "    llm_smart_compare_score = compute_llm_neprec_smart_compare(ref_entities_llm, gen_entities_llm)\n",
        "\n",
        "    return llm_smart_compare_score\n",
        "\n",
        "print(\"Function `compute_llm_neprec_extract_and_compare` defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e67c1357",
        "outputId": "e97f89c2-3aa4-46a0-a8fc-6475910b9ad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting LLM-based NePREC validations for the full dataset (210 rows). This may take some time...\n",
            "Processed 50/210 rows.\n",
            "Processed 100/210 rows.\n",
            "Processed 150/210 rows.\n",
            "Processed 200/210 rows.\n",
            "Processed 210/210 rows.\n",
            "\n",
            "--- Finished processing full dataset ---\n",
            "\n",
            "DataFrame `full_df_llm_metrics` created with all NePREC scores.\n",
            "   index   model_id  original_neprec  llm_smart_compare_neprec  \\\n",
            "0      0  gpt-4o-v2         0.083333                      0.30   \n",
            "1      1  gpt-4o-v2         0.285714                      0.50   \n",
            "2      2  gpt-4o-v2         0.142857                      0.30   \n",
            "3      3  gpt-4o-v2         0.375000                      0.55   \n",
            "4      4  gpt-4o-v2         0.833333                      0.65   \n",
            "\n",
            "   llm_extract_and_compare_neprec  llm_coverage_spacy_entities_neprec  \\\n",
            "0                            0.60                            0.222222   \n",
            "1                            0.75                            1.000000   \n",
            "2                            0.70                            0.500000   \n",
            "3                            0.85                            0.790000   \n",
            "4                            0.75                            0.560000   \n",
            "\n",
            "   llm_coverage_llm_entities_neprec  \n",
            "0                              0.86  \n",
            "1                              0.83  \n",
            "2                              0.83  \n",
            "3                              1.00  \n",
            "4                              0.70  \n"
          ]
        }
      ],
      "source": [
        "llm_validation_results_full_df = []\n",
        "\n",
        "print(f\"Starting LLM-based NePREC validations for the full dataset ({len(df)} rows). This may take some time...\")\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    ref_text = row[\"reference_text\"]\n",
        "    gen_text = row[\"generated_text\"]\n",
        "    original_neprec = row[\"neprec\"]\n",
        "    model_id = row[\"model_id\"]\n",
        "\n",
        "    # 1. Calculate LLM NePREC (semantic comparison of spaCy extracted entities)\n",
        "    ref_entities_spacy = extract_entities(ref_text)\n",
        "    gen_entities_spacy = extract_entities(gen_text)\n",
        "\n",
        "    if not gen_entities_spacy:\n",
        "        llm_smart_compare_neprec = 1.0 # No hallucination if no entities were generated by spaCy\n",
        "    else:\n",
        "        llm_smart_compare_neprec = compute_llm_neprec_smart_compare(ref_entities_spacy, gen_entities_spacy)\n",
        "\n",
        "    # 2. Calculate LLM NePREC (LLM extraction and semantic comparison)\n",
        "    llm_extract_and_compare_neprec = compute_llm_neprec_extract_and_compare(ref_text, gen_text)\n",
        "\n",
        "    # 3. Calculate LLM NePREC Coverage (using spaCy entities for comparison)\n",
        "    llm_coverage_spacy_entities_neprec = compute_llm_neprec_coverage(ref_entities_spacy, gen_entities_spacy)\n",
        "\n",
        "    # 4. Calculate LLM NePREC Coverage (using LLM extracted entities for comparison)\n",
        "    llm_coverage_llm_entities_neprec = compute_llm_neprec_coverage_llm_extracted_entities(ref_text, gen_text)\n",
        "\n",
        "    llm_validation_results_full_df.append({\n",
        "        \"index\": index,\n",
        "        \"model_id\": model_id,\n",
        "        \"original_neprec\": original_neprec,\n",
        "        \"llm_smart_compare_neprec\": llm_smart_compare_neprec,\n",
        "        \"llm_extract_and_compare_neprec\": llm_extract_and_compare_neprec,\n",
        "        \"llm_coverage_spacy_entities_neprec\": llm_coverage_spacy_entities_neprec,\n",
        "        \"llm_coverage_llm_entities_neprec\": llm_coverage_llm_entities_neprec\n",
        "    })\n",
        "\n",
        "    # Optional: Print progress for long running tasks\n",
        "    if (index + 1) % 50 == 0 or (index + 1) == len(df):\n",
        "        print(f\"Processed {index + 1}/{len(df)} rows.\")\n",
        "\n",
        "print(\"\\n--- Finished processing full dataset ---\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "full_df_llm_metrics = pd.DataFrame(llm_validation_results_full_df)\n",
        "print(\"\\nDataFrame `full_df_llm_metrics` created with all NePREC scores.\")\n",
        "print(full_df_llm_metrics.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80757e31",
        "outputId": "a2ada05b-8aef-41a5-a3a5-0c0228a43c06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               original_neprec                          llm_smart_compare_neprec                      llm_extract_and_compare_neprec                     llm_coverage_spacy_entities_neprec                     llm_coverage_llm_entities_neprec                     \n",
            "                          mean       std  min       max                     mean       std  min   max                           mean       std  min  max                               mean       std  min  max                             mean       std   min  max\n",
            "model_id                                                                                                                                                                                                                                                             \n",
            "gpt-4o-mini-v2        0.289932  0.254560  0.0  1.000000                 0.403524  0.190959  0.0  0.75                       0.594762  0.182125  0.2  1.0                           0.405733  0.247466  0.0  1.0                         0.773309  0.235469  0.22  1.0\n",
            "gpt-4o-v2             0.231257  0.185116  0.0  0.833333                 0.479605  0.166902  0.0  0.85                       0.626190  0.156907  0.2  1.0                           0.654078  0.237791  0.0  1.0                         0.836420  0.199440  0.12  1.0\n"
          ]
        }
      ],
      "source": [
        "print(full_df_llm_metrics.groupby(\"model_id\")[[\n",
        "    \"original_neprec\",\n",
        "    \"llm_smart_compare_neprec\",\n",
        "    \"llm_extract_and_compare_neprec\",\n",
        "    \"llm_coverage_spacy_entities_neprec\",\n",
        "    \"llm_coverage_llm_entities_neprec\"\n",
        "]].agg(['mean', 'std', 'min', 'max']).to_string())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
